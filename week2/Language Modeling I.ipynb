{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Language Modeling I\n",
    "\n",
    "In this week's section, we'll explore basic techniques for language modeling. \n",
    "\n",
    "We'll first dig into word distributions to illustrate the sparsity problem. Then, we'll see how to build an N-gram language model, how to generate sample sentences, and how to use smoothing to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [Plotly](https://plot.ly) for plotting some data in this notebook. They're a commercial service, but the plotting library is open-source and provides a much more modern, high-level interface than `matplotlib`. Install it with:\n",
    "```\n",
    "pip install plotly\n",
    "```\n",
    "You can check out their documentation [here](https://plot.ly/python/), but the code in this notebook should be self-explanatory if you're familiar with `matplotlib`, `ggplot`, or `bokeh`.\n",
    "\n",
    "If you get an import error below, go to **Kernel -> Restart** after running the `pip` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d all\n",
      "    Downloading collection u'all'\n",
      "       | \n",
      "       | Downloading package abc to /root/nltk_data...\n",
      "       |   Unzipping corpora/abc.zip.\n",
      "       | Downloading package alpino to /root/nltk_data...\n",
      "       |   Unzipping corpora/alpino.zip.\n",
      "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
      "       |   Unzipping corpora/biocreative_ppi.zip.\n",
      "       | Downloading package brown to /root/nltk_data...\n",
      "       |   Unzipping corpora/brown.zip.\n",
      "       | Downloading package brown_tei to /root/nltk_data...\n",
      "       |   Unzipping corpora/brown_tei.zip.\n",
      "       | Downloading package cess_cat to /root/nltk_data...\n",
      "       |   Unzipping corpora/cess_cat.zip.\n",
      "       | Downloading package cess_esp to /root/nltk_data...\n",
      "       |   Unzipping corpora/cess_esp.zip.\n",
      "       | Downloading package chat80 to /root/nltk_data...\n",
      "       |   Unzipping corpora/chat80.zip.\n",
      "       | Downloading package city_database to /root/nltk_data...\n",
      "       |   Unzipping corpora/city_database.zip.\n",
      "       | Downloading package cmudict to /root/nltk_data...\n",
      "       |   Unzipping corpora/cmudict.zip.\n",
      "       | Downloading package comparative_sentences to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping corpora/comparative_sentences.zip.\n",
      "       | Downloading package comtrans to /root/nltk_data...\n",
      "       | Downloading package conll2000 to /root/nltk_data...\n",
      "       |   Unzipping corpora/conll2000.zip.\n",
      "       | Downloading package conll2002 to /root/nltk_data...\n",
      "       |   Unzipping corpora/conll2002.zip.\n",
      "       | Downloading package conll2007 to /root/nltk_data...\n",
      "       | Downloading package crubadan to /root/nltk_data...\n",
      "       |   Unzipping corpora/crubadan.zip.\n",
      "       | Downloading package dependency_treebank to /root/nltk_data...\n",
      "       |   Unzipping corpora/dependency_treebank.zip.\n",
      "       | Downloading package europarl_raw to /root/nltk_data...\n",
      "       |   Unzipping corpora/europarl_raw.zip.\n",
      "       | Downloading package floresta to /root/nltk_data...\n",
      "       |   Unzipping corpora/floresta.zip.\n",
      "       | Downloading package framenet_v15 to /root/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v15.zip.\n",
      "       | Downloading package gazetteers to /root/nltk_data...\n",
      "       |   Unzipping corpora/gazetteers.zip.\n",
      "       | Downloading package genesis to /root/nltk_data...\n",
      "       |   Unzipping corpora/genesis.zip.\n",
      "       | Downloading package gutenberg to /root/nltk_data...\n",
      "       |   Unzipping corpora/gutenberg.zip.\n",
      "       | Downloading package ieer to /root/nltk_data...\n",
      "       |   Unzipping corpora/ieer.zip.\n",
      "       | Downloading package inaugural to /root/nltk_data...\n",
      "       |   Unzipping corpora/inaugural.zip.\n",
      "       | Downloading package indian to /root/nltk_data...\n",
      "       |   Unzipping corpora/indian.zip.\n",
      "       | Downloading package jeita to /root/nltk_data...\n",
      "       | Downloading package kimmo to /root/nltk_data...\n",
      "       |   Unzipping corpora/kimmo.zip.\n",
      "       | Downloading package knbc to /root/nltk_data...\n",
      "       | Downloading package lin_thesaurus to /root/nltk_data..."
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade plotly\n",
    "#!pip install --upgrade nltk\n",
    "import os, sys, re, json, time\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download()\n",
    "# Helper libraries for this notebook\n",
    "import data_utils\n",
    "import vocabulary\n",
    "\n",
    "# Plotly\n",
    "import plotly.offline as plotly\n",
    "plotly.offline.init_notebook_mode()\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics\n",
    "\n",
    "NLTK includes a number of corpora that we can experiment with for this exercise. Different types of text can have very different N-gram distributions, and some are more difficult to model than others.\n",
    "\n",
    "Let's start with the [Brown corpus](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html), the first major computer-readable linguistic corpus. It consists of around 1 million words of American English, sampled from 15 different text categories ranging from news text to academic articles to popular fiction.\n",
    "\n",
    "**Note:** If you get an error loading this or any other corpus through NLTK, run `nltk.download()` and follow the instructions to get the data. You'll probably want to select \"`all`\" or \"`all corpora`\", and then you'll be (mostly) set for the semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57340 sentences (1.16119e+06 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the Brown corpus\n",
    "sentences = list(nltk.corpus.brown.sents())\n",
    "print \"Loaded %d sentences (%g tokens)\" % (len(sentences), sum(map(len, sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    return itertools.chain.from_iterable(list_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the all the words in the corpus, and looking at some basic statistics.\n",
    "\n",
    "We've built a helper class, `Vocabulary`, that ingests a list of words, counts their frequencies, and assigns each one a numerical ID that will be useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 48174\n",
      "Most common unigrams:\n",
      "\"the\": 69971\n",
      "\",\": 58334\n",
      "\".\": 49346\n",
      "\"of\": 36412\n",
      "\"and\": 28853\n",
      "\"to\": 26158\n",
      "\"a\": 23195\n",
      "\"in\": 21337\n",
      "\"that\": 10594\n",
      "\"is\": 10109\n"
     ]
    }
   ],
   "source": [
    "canonicalize = True  # if so, will make everything lowercase and canonicalize digits\n",
    "\n",
    "if canonicalize:\n",
    "    token_feed = (data_utils.canonicalize_word(w) for w in flatten(sentences))\n",
    "else:\n",
    "    token_feed = flatten(sentences)\n",
    "\n",
    "vocab = vocabulary.Vocabulary(token_feed)\n",
    "print \"Vocabulary size: %d\" % vocab.size\n",
    "\n",
    "print \"Most common unigrams:\"\n",
    "for word, count in vocab.unigram_counts.most_common(10):\n",
    "    print \"\\\"%s\\\": %d\" % (word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Vocabulary.unigram_counts` is a dictionary (actually, `collections.Counter`) of the unigram frequencies $c(w)$. Let's look at a plot of the top frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"ef17b936-c209-4026-b86f-b289a6ff95ad\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ef17b936-c209-4026-b86f-b289a6ff95ad\", [{\"y\": [69971, 58334, 49346, 36412, 28853, 26158, 23195, 21337, 10594, 10109, 9815, 9548, 9489, 8837, 8789, 8760, 7289, 7253, 6996, 6741], \"x\": [\"the\", \",\", \".\", \"of\", \"and\", \"to\", \"a\", \"in\", \"that\", \"is\", \"was\", \"he\", \"for\", \"``\", \"''\", \"it\", \"with\", \"as\", \"his\", \"on\"], \"type\": \"bar\", \"name\": \"Counts\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words, counts = zip(*vocab.unigram_counts.most_common(20))\n",
    "data = [go.Bar(x=words, y=counts, name='Counts')]\n",
    "plotly.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that it falls off very quickly! Recall that word frequencies tend to follow **Zipf's law**, in that they are rougly proportional to $\\frac{1}{\\mathrm{rank}(w)}$, where rank = 1 for the most common word, 2 for the second-most, etc. We can test this directly with a numerical fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power law exponent: β = -1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in log\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"da84d80c-e1a3-4e4b-a530-fcf8b88b502e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"da84d80c-e1a3-4e4b-a530-fcf8b88b502e\", [{\"y\": [69971.0, 58334.0, 49346.0, 36412.0, 28853.0, 26158.0, 23195.0, 21337.0, 10594.0, 10109.0, 9815.0, 9548.0, 9489.0, 8837.0, 8789.0, 8760.0, 7289.0, 7253.0, 6996.0, 6741.0, 6377.0, 5566.0, 5372.0, 5306.0, 5164.0, 5145.0, 5133.0, 4693.0, 4610.0, 4394.0, 4381.0, 4370.0, 4206.0, 3942.0, 3740.0, 3620.0, 3561.0, 3432.0, 3292.0, 3286.0, 3284.0, 3036.0, 3001.0, 2860.0, 2728.0, 2714.0, 2669.0, 2652.0, 2619.0, 2472.0, 2466.0, 2437.0, 2435.0, 2331.0, 2252.0, 2245.0, 2215.0, 2198.0, 2139.0, 2097.0, 2006.0, 1985.0, 1961.0, 1928.0, 1915.0, 1908.0, 1890.0, 1858.0, 1815.0, 1795.0, 1791.0, 1790.0, 1788.0, 1772.0, 1748.0, 1702.0, 1635.0, 1618.0, 1601.0, 1598.0, 1596.0, 1573.0, 1412.0, 1402.0, 1380.0, 1363.0, 1361.0, 1344.0, 1318.0, 1314.0, 1303.0, 1292.0, 1252.0, 1236.0, 1207.0, 1181.0, 1170.0, 1159.0, 1125.0, 1069.0], \"x\": [\"the\", \",\", \".\", \"of\", \"and\", \"to\", \"a\", \"in\", \"that\", \"is\", \"was\", \"he\", \"for\", \"``\", \"''\", \"it\", \"with\", \"as\", \"his\", \"on\", \"be\", \";\", \"at\", \"by\", \"i\", \"this\", \"had\", \"?\", \"not\", \"are\", \"but\", \"from\", \"or\", \"have\", \"an\", \"they\", \"which\", \"--\", \"one\", \"you\", \"were\", \"her\", \"all\", \"she\", \"there\", \"would\", \"their\", \"we\", \"him\", \"been\", \")\", \"has\", \"(\", \"when\", \"who\", \"will\", \"more\", \"if\", \"no\", \"out\", \"DG\", \"so\", \"said\", \"DGDGDGDG\", \"DGDG\", \"what\", \"up\", \"its\", \"about\", \":\", \"into\", \"than\", \"them\", \"can\", \"only\", \"other\", \"new\", \"some\", \"could\", \"time\", \"!\", \"these\", \"two\", \"may\", \"then\", \"do\", \"first\", \"any\", \"my\", \"now\", \"such\", \"like\", \"our\", \"over\", \"man\", \"me\", \"even\", \"most\", \"made\", \"also\"], \"type\": \"bar\", \"name\": \"Counts\"}, {\"y\": [232624.0, 87358.0, 49259.0, 32806.0, 23934.0, 18498.0, 14878.0, 12320.0, 10431.0, 8988.0, 7856.0, 6947.0, 6204.0, 5587.0, 5068.0, 4626.0, 4247.0, 3917.0, 3629.0, 3375.0, 3150.0, 2950.0, 2770.0, 2609.0, 2463.0, 2330.0, 2209.0, 2098.0, 1997.0, 1903.0, 1817.0, 1737.0, 1663.0, 1595.0, 1531.0, 1471.0, 1415.0, 1363.0, 1314.0, 1268.0, 1224.0, 1183.0, 1144.0, 1108.0, 1073.0, 1040.0, 1009.0, 980.0, 952.0, 925.0, 899.0, 875.0, 852.0, 829.0, 808.0, 788.0, 768.0, 750.0, 732.0, 715.0, 698.0, 682.0, 667.0, 652.0, 638.0, 625.0, 612.0, 599.0, 587.0, 575.0, 563.0, 552.0, 542.0, 531.0, 521.0, 512.0, 502.0, 493.0, 485.0, 476.0, 468.0, 460.0, 452.0, 444.0, 437.0, 430.0, 423.0, 416.0, 409.0, 403.0, 397.0, 391.0, 385.0, 379.0, 373.0, 368.0, 363.0, 357.0, 352.0, 347.0], \"x\": [\"the\", \",\", \".\", \"of\", \"and\", \"to\", \"a\", \"in\", \"that\", \"is\", \"was\", \"he\", \"for\", \"``\", \"''\", \"it\", \"with\", \"as\", \"his\", \"on\", \"be\", \";\", \"at\", \"by\", \"i\", \"this\", \"had\", \"?\", \"not\", \"are\", \"but\", \"from\", \"or\", \"have\", \"an\", \"they\", \"which\", \"--\", \"one\", \"you\", \"were\", \"her\", \"all\", \"she\", \"there\", \"would\", \"their\", \"we\", \"him\", \"been\", \")\", \"has\", \"(\", \"when\", \"who\", \"will\", \"more\", \"if\", \"no\", \"out\", \"DG\", \"so\", \"said\", \"DGDGDGDG\", \"DGDG\", \"what\", \"up\", \"its\", \"about\", \":\", \"into\", \"than\", \"them\", \"can\", \"only\", \"other\", \"new\", \"some\", \"could\", \"time\", \"!\", \"these\", \"two\", \"may\", \"then\", \"do\", \"first\", \"any\", \"my\", \"now\", \"such\", \"like\", \"our\", \"over\", \"man\", \"me\", \"even\", \"most\", \"made\", \"also\"], \"type\": \"scatter\", \"name\": \"Zipf Fit\"}], {\"yaxis\": {\"range\": [0, 83965.2]}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words, counts = zip(*vocab.unigram_counts.most_common(100))\n",
    "counts = np.array(counts, dtype=float)\n",
    "rank = 1 + np.arange(len(counts))\n",
    "N = np.sum(counts)\n",
    "p = counts / N\n",
    "\n",
    "# Optimize least-squares in log space\n",
    "# see http://nlp.stanford.edu/IR-book/html/htmledition/zipfs-law-modeling-the-distribution-of-terms-1.html\n",
    "import scipy.optimize\n",
    "fit_func = lambda (a, b): (np.log(a*p) - np.log(a * rank**b))\n",
    "(a,b), _ = scipy.optimize.leastsq(fit_func, np.array([p[0], -1.0]))\n",
    "print u\"Power law exponent: \\u03B2 = %.02f\" % b\n",
    "p_pred = (a * rank**b) / sum(a * rank**b)  # predict probabilities\n",
    "c_pred = N * p_pred  # predict counts\n",
    "\n",
    "# Plot counts, with fit curve\n",
    "data = [go.Bar(x=words, y=counts, name='Counts'),\n",
    "        go.Scatter(x=words, y=np.round(c_pred), name=\"Zipf Fit\")]\n",
    "layout=go.Layout(yaxis=dict(range=[0,1.2*max(counts)]))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Language Models\n",
    "\n",
    "If we want, we can take our unigram counts and use them as a language model directly:\n",
    "$$ P(w_i | w_{i-1}, ..., w_0) = P(w_i) $$\n",
    "\n",
    "Here's a quick implementation. We don't need all the class machinery, but it's easier to see the structure that way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramLM(object):\n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"Build the model.\"\"\"\n",
    "        self.words = vocab.word_to_id.keys()\n",
    "        self.counts = np.array([vocab.unigram_counts[w] \n",
    "                                for w in self.words], dtype=float)\n",
    "        self.probs = self.counts / sum(self.counts)\n",
    "        # For looking up words when scoring\n",
    "        self._word_to_idx = {w:i for i,w in enumerate(self.words)}\n",
    "        \n",
    "    def predict_next(self, seq):\n",
    "        \"\"\"Predict the next word in the sequence.\"\"\"\n",
    "        word = np.random.choice(self.words, p=self.probs)\n",
    "        return word\n",
    "    \n",
    "    def score_seq(self, seq):\n",
    "        \"\"\"Score a sequence; returns estimated log probability.\"\"\"\n",
    "        score = 0.0\n",
    "        for word in seq:\n",
    "            idx = self._word_to_idx[word]\n",
    "            score += np.log2(self.probs[idx])\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate a few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "came had my worked after `` scattered for or papa know with but they delight , philanthropic thum have normal\n",
      "[20 tokens; log P(seq): -217.59]\n",
      "\n",
      "of possible does with one's the i of `` sounds , the to of bargaining .\n",
      "[16 tokens; log P(seq): -126.89]\n",
      "\n",
      "unenvied as somewhat oriental base most mg. `` that .\n",
      "[10 tokens; log P(seq): -115.44]\n",
      "\n",
      ".\n",
      "[1 tokens; log P(seq): -4.56]\n",
      "\n",
      "case in out corpse on don top an i elders ? headline swells their dialogue a st. turned these maye\n",
      "[20 tokens; log P(seq): -240.97]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm1 = UnigramLM(vocab)\n",
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = []\n",
    "    for i in range(max_length):\n",
    "        seq.append(lm1.predict_next(seq))\n",
    "        # Stop when we generate a period\n",
    "        if seq[-1] == \".\": break\n",
    "    print \" \".join(seq)\n",
    "    print \"[%d tokens; log P(seq): %.02f]\" % (len(seq), lm1.score_seq(seq))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus:** What is the *most likely* sequence that this unigram model could produce?\n",
    "\n",
    "**Bonus:** How likely is this model to emit a period at each step? What is the expected length of a sentence? How are the lengths distributed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model\n",
    "\n",
    "The unigram model isn't very impressive - mostly, it just gives gibberish consisting of common words. Let's build a better model from bigrams:\n",
    "$$ P(w_i | w_{i-1}, ..., w_0) = P(w_i | w_{i-1}) $$\n",
    "\n",
    "**Caution!** We have a big vocabulary. How many possible bigrams are there? \n",
    "\n",
    "If we store it as a matrix $P_{ab} = P(w_i = b | w_{i-1} = a)$, how big a matrix will this be, in megabytes? (assume 8 bytes per entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needed:    17705 MB\n",
      "Available: 424 MB\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade psutil\n",
    "import psutil\n",
    "print \"Needed:    %g MB\" % (8 * (vocab.size ** 2) / (2**20))\n",
    "print \"Available: %g MB\" % (psutil.virtual_memory().available / (2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's no good. Even for a small vocabulary, we can't store all the possible bigrams.\n",
    "\n",
    "**Q:** What if we consider all the possible bigrams *that exist in the corpus*? How many are there, at most?\n",
    "\n",
    "Let's construct a language model that only stores observed bigrams. We'll have to do a little more work to deal with unobserved bigrams, but it'll be worth the memory savings.\n",
    "\n",
    "We'll compute the probabilities by usual formula, starting with raw bigram counts:\n",
    "\n",
    "$$  P_{ab} = P(w_i = b | w_{i-1} = a) = \\frac{\\mathrm{c(ab)}}{\\sum_{a'}\\mathrm{c(a'b)}} = \\frac{C_{ab}}{\\sum_{a'} C_{a'b}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def normalize_counter(c):\n",
    "    total = sum(c.itervalues())\n",
    "    return {w:float(c[w])/total for w in c}\n",
    "\n",
    "class SimpleBigramLM(object):\n",
    "    def __init__(self, words):\n",
    "        # We'll make this a defaultdict to simplify\n",
    "        # adding new words as we see them.\n",
    "        # counts will map {a: {b: count(a,b)}}\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "        \n",
    "        # Iterate through words, as a single pass\n",
    "        # This way, we allow words to be a generator stream,\n",
    "        # instead of needing it to all be in-memory.\n",
    "        a_ = None\n",
    "        for b_ in words:\n",
    "            if a_ is not None:\n",
    "                self.bigram_counts[a_][b_] += 1\n",
    "            a_ = b_\n",
    "            \n",
    "        # Normalize to get P(b | a)\n",
    "        self.bigram_probs = {a:normalize_counter(c) \n",
    "                             for a,c in self.bigram_counts.iteritems()}\n",
    "    \n",
    "    def predict_next(self, seq):\n",
    "        if len(seq) == 0:\n",
    "            raise ValueError(\"Simple Bigram LM needs context of at least 1 word!\")\n",
    "        # Get list of words, probs\n",
    "        words, probs = zip(*self.bigram_probs[seq[-1]].items())\n",
    "        return np.random.choice(words, p=probs)\n",
    "    \n",
    "    def score_seq(self, seq):\n",
    "        # Score all bigrams\n",
    "        score = 0.0\n",
    "        for i in range(1, len(seq)):\n",
    "            a,b = seq[i-1], seq[i]\n",
    "            score += np.log2(self.bigram_probs[a][b])\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't do this for the unigram LM, but when modeling sentences it's helpful to add special beginning-of-sentence (`<s>`) and end-of-sentence (`</s>`) tokens. \n",
    "\n",
    "This lets the model estimate the probability of a word appearing at the beginning of a sentence, and lets it model the end of a sentence properly, since periods or other punctuation aren't always an accurate guide (e.g. `\"Dr.\"` or `\"Yahoo!\"`).\n",
    "\n",
    "Our padded sentences will look like this:\n",
    "```\n",
    "<s> the cat sat in the hat . </s>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building bigram LM... done in 2.64 s\n"
     ]
    }
   ],
   "source": [
    "padded_sentences = ([\"<s>\"] + s + [\"</s>\"] for s in sentences)\n",
    "token_feed = (data_utils.canonicalize_word(w) \n",
    "              for w in itertools.chain.from_iterable(padded_sentences))\n",
    "t0 = time.time()\n",
    "print \"Building bigram LM...\",\n",
    "lm2 = SimpleBigramLM(token_feed)\n",
    "print \"done in %.02f s\" % (time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 429128 bigrams in 1.27587e+06 tokens (48173 unique words)\n",
      "Optimal memory usage: 9 MB\n"
     ]
    }
   ],
   "source": [
    "unique_unigrams = len(lm2.bigram_probs)\n",
    "unique_bigrams = sum(len(c) for c in lm2.bigram_probs.itervalues())\n",
    "total_words = sum(sum(c.values()) for c in lm2.bigram_counts.itervalues())\n",
    "print \"Found %g bigrams in %g tokens (%g unique words)\" % (unique_bigrams, total_words, unique_unigrams)\n",
    "\n",
    "# Compute memory use, if we stored them in the most efficient way\n",
    "# Assume 4 bytes for index a, 4 bytes for index b, 8 bytes for P_ab\n",
    "# and 8 bytes of overhead (e.g. for pointers)\n",
    "print \"Optimal memory usage: %d MB\" % ((unique_bigrams * 24) / (2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate sentences, much as we did with the unigram LM. They should look a little better this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> -- 16-mesh screening of newbury shore . </s>\n",
      "[9 tokens; log P(seq): -41.86]\n",
      "\n",
      "<s> another cure made evident than most fundamental law and instead we '' ? </s>\n",
      "[15 tokens; log P(seq): -94.77]\n",
      "\n",
      "<s> she would i asked tiredly repeat . </s>\n",
      "[9 tokens; log P(seq): -38.46]\n",
      "\n",
      "<s> well , fastened to the conductor '' . </s>\n",
      "[10 tokens; log P(seq): -51.74]\n",
      "\n",
      "<s> roberta said the music . </s>\n",
      "[7 tokens; log P(seq): -34.10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(lm2.predict_next(seq))\n",
    "        # Stop at end-of-sentence\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print \" \".join(seq)\n",
    "    print \"[%d tokens; log P(seq): %.02f]\" % (len(seq), lm2.score_seq(seq))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much better than the unigram output! The generated sentences are a bit more grammatical, but still don't make much sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing and Handling the Unknown\n",
    "\n",
    "Our simple bigram model doesn't have any mechanism for handling unknown words - in fact, if we fed something unseen to `score_seq`, it would just crash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'w266'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a9937d7bfcb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlm2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"<s>\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"love\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w266\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"</s>\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-0d743213d58f>\u001b[0m in \u001b[0;36mscore_seq\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbigram_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'w266'"
     ]
    }
   ],
   "source": [
    "lm2.score_seq([\"<s>\", \"i\", \"love\", \"w266\", \"</s>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just default to $ P(\\texttt{<unk>} | w_{i-1}) = 0 $, but then we'd assign an implausibly low probability to anything we haven't seen before.\n",
    "\n",
    "If we want to use our language model in the wild, we'll need to implement smoothing. Of course, there's lots of different ways to do this, and it's going to introduce hyperparameters we'll want to tune. So, we should also split off some of our data into a dev/validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57340 sentences (1.16119e+06 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load a corpus.\n",
    "# After you have things working, try changing \"brown\" to \"state_union\" or \"gutenberg\"\n",
    "# You will also want to change the vocab size; 30k for brown, 10k for state_union, 20k for gutenberg.\n",
    "# See http://www.nltk.org/nltk_data/ for a full list; you may need to use nltk.download() again.\n",
    "sentences = list(nltk.corpus.brown.sents())\n",
    "vocab_size = 30000\n",
    "print \"Loaded %d sentences (%g tokens)\" % (len(sentences), sum(map(len, sentences)))\n",
    "\n",
    "train_frac = 0.8\n",
    "split_idx = int(train_frac * len(sentences))\n",
    "train_sentences = sentences[:split_idx]\n",
    "dev_sentences = sentences[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a smoothed trigram model. We'll build a single model that can do several different types of smoothing:\n",
    "\n",
    "- Laplace / add-k smoothing\n",
    "- Absolute discounting\n",
    "- Backoff smoothing\n",
    "- Interpolation\n",
    "- Type fertility\n",
    "\n",
    "The code for this is a bit lengthy, so we've moved it to **`ngram_lm.py`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM... done in 5.09 s\n"
     ]
    }
   ],
   "source": [
    "# We'll use this to clip the vocabulary to the top words\n",
    "token_feed = (data_utils.canonicalize_word(w) \n",
    "              for w in itertools.chain.from_iterable(sentences))\n",
    "vocab_pruned = vocabulary.Vocabulary(token_feed, size=vocab_size)\n",
    "\n",
    "padded_sentences = ([\"<s>\", \"<s>\"] + s + [\"</s>\"] for s in train_sentences)\n",
    "token_feed = (data_utils.canonicalize_word(w, wordset=vocab_pruned.word_to_id) \n",
    "              for w in itertools.chain.from_iterable(padded_sentences))\n",
    "\n",
    "import ngram_lm\n",
    "reload(ngram_lm)\n",
    "t0 = time.time()\n",
    "print \"Building trigram LM...\",\n",
    "lm3 = ngram_lm.SmoothedTrigramLM(token_feed)\n",
    "print \"done in %.02f s\" % (time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3.35178e+06 tokens. Found:\n",
      "  28706 unique unigrams\n",
      "  356137 unique bigrams\n",
      "  731783 unique trigrams\n",
      "Optimal memory usage: 28 MB\n"
     ]
    }
   ],
   "source": [
    "unique_unigrams = sum(len(c) for k,c in lm3.counts.iteritems()\n",
    "                      if len(k) == 0)\n",
    "unique_bigrams  = sum(len(c) for k,c in lm3.counts.iteritems()\n",
    "                      if len(k) == 1)\n",
    "unique_trigrams = sum(len(c) for k,c in lm3.counts.iteritems()\n",
    "                      if len(k) == 2)\n",
    "total_words = sum(sum(c.values()) for c in lm3.counts.itervalues())\n",
    "print \"Processed %g tokens. Found:\" % total_words\n",
    "print \"  %g unique unigrams\" % unique_unigrams\n",
    "print \"  %g unique bigrams\" % unique_bigrams\n",
    "print \"  %g unique trigrams\" % unique_trigrams\n",
    "\n",
    "# Compute memory use, if we stored them in the most efficient way\n",
    "# Assume 4 bytes for index a, 4 bytes for index b, 8 bytes for P_ab\n",
    "# and 8 bytes of overhead (e.g. for pointers)\n",
    "total_memory = (unique_unigrams * 20) + (unique_bigrams * 24) + (unique_trigrams * 28)\n",
    "print \"Optimal memory usage: %d MB\" % (total_memory / (2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate sentences, as with the previous models! This will be a bit slow, as we don't precompute any of the smoothing calculations. *(We could be a lot faster by precomputing $ P(w_i | w_{i-1}, w_{i-2}) $ for all nonzero trigram counts and so on with the bigrams and unigrams, but this will do for now.)*\n",
    "\n",
    "Change the parameters below to change the type of smoothing. The default is to use a KN model with $\\delta = 0.75$, but you might also want to try un-commenting some of the others below.\n",
    "\n",
    "**Q:** What do you notice about the per-token log probability on a smoothed model, versus an un-smoothed one? \n",
    "\n",
    "**Q:** Which model gives more plausible sentences? Do you expect that this model will generalize well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set options here:\n",
    "#   kn: true/false\n",
    "#   delta: float\n",
    "#   backoff: int (minimum count)\n",
    "#   interpolation: (l1,l2,l3) (should sum to 1)\n",
    "#   add_k: float\n",
    "params = dict(kn=True, delta=0.75) # KN smoothing\n",
    "# params = dict(kn=False, add_k=0.0) # unsmoothed!\n",
    "# params = dict(kn=False, add_k=1.0) # add-1 (Laplace) smoothing\n",
    "# params = dict(kn=False, add_k=1.0, interpolation=(0.2, 0.3, 0.5))\n",
    "# params = dict(kn=False, add_k=1.0, backoff=5) # Katz smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_length = 30\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]  # start with two to init trigram model\n",
    "    for i in range(max_length):\n",
    "        seq.append(lm3.predict_next(seq, **params))\n",
    "        # Stop at end-of-sentence\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print \" \".join(seq)\n",
    "    score = lm3.score_seq(seq, **params)\n",
    "    print \"[%d tokens; log P(seq): %.02f, per-token: %.02f]\" % (len(seq), score, \n",
    "                                                                score/(len(seq)-2))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity Measurement\n",
    "\n",
    "Let's not forget the test set! The whole point of smoothing was do better on unseen data.\n",
    "\n",
    "Which model performs best? Experiment with the parameters a bit to tune your LM.\n",
    "\n",
    "**Q:** What happens if you use an unsmoothed model on this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params = dict(kn=True, delta=0.75) # KN smoothing\n",
    "params = dict(kn=False, add_k=0.0) # unsmoothed!\n",
    "# params = dict(kn=False, add_k=1.0) # add-1 (Laplace) smoothing\n",
    "# params = dict(kn=False, add_k=1.0, interpolation=(0.2, 0.3, 0.5))\n",
    "# params = dict(kn=False, add_k=1.0, backoff=5) # Katz smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored dev set in 1.73 s\n",
      "Average log score: -inf\n",
      "Perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "total_score = 0.0\n",
    "total_tokens = 0\n",
    "\n",
    "def preprocess_for_scoring(sentence):\n",
    "    # Pre-process words, replace anything the model doesn't know\n",
    "    # with <unk>\n",
    "    words = [data_utils.canonicalize_word(w, wordset=known_words)\n",
    "             for w in sentence]\n",
    "    # Pad sequence with start and end markers\n",
    "    return [\"<s>\", \"<s>\"] + words + [\"</s>\"]\n",
    "\n",
    "known_words = set(lm3.words)\n",
    "t0 = time.time()\n",
    "for sentence in dev_sentences:\n",
    "    seq = preprocess_for_scoring(sentence)\n",
    "    score = lm3.score_seq(seq, **params)\n",
    "    total_score += score\n",
    "    total_tokens += (len(seq) - 2)\n",
    "    \n",
    "print \"Scored dev set in %.02f s\" % (time.time() - t0)\n",
    "print \"Average log score: %.02f\" % (total_score / total_tokens)\n",
    "print \"Perplexity: %.02f\" % (2**(-1*total_score/total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Curiosities\n",
    "\n",
    "You might have seen this floating around the internet last week:\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "Let's see if it holds true, statistically at least. Note that log probabilities are always negative, so the smaller magnitude is better. And remember the log scale: a difference of score of 8 units means one utterance is $2^8 = 256$ times more likely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s0 score: -62.12\n",
      "s1 score: -70.78\n"
     ]
    }
   ],
   "source": [
    "s0 = preprocess_for_scoring(\"square green plastic toys\".split())\n",
    "s1 = preprocess_for_scoring(\"plastic green square toys\".split())\n",
    "\n",
    "params = dict(kn=True, delta=0.75)\n",
    "print \"s0 score: %.02f\" % lm3.score_seq(s0, **params)\n",
    "print \"s1 score: %.02f\" % lm3.score_seq(s1, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a more scientific comparison, by looking at all possible permutations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"square green plastic toys\" : -62.12\n",
      "\"green square plastic toys\" : -62.15\n",
      "\"plastic square green toys\" : -70.78\n",
      "\"plastic green square toys\" : -70.78\n",
      "\"square plastic green toys\" : -71.00\n",
      "\"green plastic square toys\" : -71.03\n"
     ]
    }
   ],
   "source": [
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "results = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = list(adjs) + [noun]\n",
    "    seq = preprocess_for_scoring(words)\n",
    "    score = lm3.score_seq(seq, **params)\n",
    "    results.append((score, words))\n",
    "\n",
    "# Sort results\n",
    "for score, words in sorted(results, reverse=True):\n",
    "    print \"\\\"%s\\\" : %.02f\" % (\" \".join(words), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
