{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Set-Up\n",
    "\n",
    "Welcome to MIDS W266: Natural Language Processing! \n",
    "\n",
    "This notebook is a quick guide to getting set up with the programming environment we'll be using this semester. We'll be using IPython (Jupyter) notebooks for most of the course exercises and assignments, and we'll make heavy use of NumPy, scikit-learn, TensorFlow, and NLTK. \n",
    "\n",
    "The instructions below should get you set up for most of the semester, although since this is the first time this course is offered we might add a few new things as we go along.\n",
    "\n",
    "We'll do our best to support a variety of platforms, but most NLP software (and Data Science software in general) works best on UNIX-based operating systems, i.e. **Linux** or **Mac OSX**. While you'll be doing most coding in Python, it'll be very useful to be familiar with the **bash** command-line environment and common utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Instance set-up\n",
    "\n",
    "If you plan on using a GCE instance for the course, follow the instructions on the [Cloud Instance Guide](https://github.com/datasci-w266/main/blob/master/cloud/README.md). This will walk you through cloning the git repo, installing Anaconda and TensorFlow, and setting up to use Jupyter notebooks.\n",
    "\n",
    "When you're done, run a notebook server on your cloud instance:\n",
    "```\n",
    "cd ~\n",
    "jupyter notebook\n",
    "```\n",
    "And in your browser, navigate to http://localhost:8888/notebooks/w266/week1/Course%20Set-Up.ipynb to load a live version of this notebook. (_If you cloned to a different folder than `~/W266`, you might need to change this URL, or browse from the [tree view](http://localhost:8888/tree)_)\n",
    "\n",
    "You can skip the local set-up section and jump down to the **NLTK** part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local set-up\n",
    "\n",
    "If you plan on working on your local machine, first install `git` and clone the course repo:\n",
    "```\n",
    "git clone https://github.com/datasci-w266/main.git ~/w266\n",
    "```\n",
    "Or if you have authentication issues:\n",
    "```\n",
    "git clone git@github.com:datasci-w266/main.git ~/w266\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local set-up: Python\n",
    "\n",
    "We strongly recommend the [**Anaconda**](https://www.continuum.io/downloads) python distribution, which includes NumPy, scikit-learn, matplotlib, pandas, NLTK, and many other useful packages. For most code, we'll assume that you have Anaconda, and mention explicitly anything else that's not included.\n",
    "\n",
    "Download the Python 2.7 version from https://www.continuum.io/downloads, and follow the instructions to install.\n",
    "\n",
    "There are a few Python features that we'll be making use of that you might not have encounted in previous courses. You might want to bookmark these, and take a glance at the documentation for these now - although we'll explain them more as the appear.\n",
    "\n",
    "- [Generators and generator expressions](https://wiki.python.org/moin/Generators), handy for working with streams of text\n",
    "- [SciPy sparse matricies](http://docs.scipy.org/doc/scipy/reference/sparse.html) for representing large \"one hot\" vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local set-up: TensorFlow\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) is Google's open-source numerical computation library. It's designed for deep learning, and has very good support for the neural network architectures, such as RNNs, that are commonly used in NLP.\n",
    "\n",
    "**Note:** TensorFlow is only available for Linux and OSX. If you're on Windows, the easiest option is to use a Google Cloud instance. See the [Cloud Instance Guide](https://github.com/datasci-w266/main/cloud/README.md) for more details.\n",
    "\n",
    "If you're using Anaconda as above, you can install the latest version of TensorFlow with:\n",
    "```\n",
    "conda install -c jjhelmus tensorflow\n",
    "```\n",
    "\n",
    "Alternatively, you can follow the Pip Installation instructions here: https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#pip-installation (Ignore everything about conda or virtualenv environments.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: TensorFlow and GPUs\n",
    "\n",
    "TensorFlow can use a GPU to dramatically accelerate running neural network models. If you have a recent NVidia GPU, follow the instructions here to get it set up:\n",
    "https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#optional-linux-enable-gpu-support\n",
    "\n",
    "Be warned that CUDA and NVidia drivers on Linux can be finicky and sometimes unstable, so consult the course staff if you plan on going this route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Local set-up: Notebooks\n",
    "\n",
    "If the above steps completed successfully, you should be able to open a notebook with:\n",
    "```\n",
    "cd ~\n",
    "jupyter notebook &\n",
    "```\n",
    "It should open a browser window to http://localhost:8888/tree; find this notebook and open it to continue. You can also try the direct link:\n",
    "- http://localhost:8888/notebooks/w266/week1/Course%20Set-Up.ipynb\n",
    "\n",
    "Run the cells below to test your Python installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "Welcome to Natural Language Processing!\n"
     ]
    }
   ],
   "source": [
    "print \"Hello world!\"\n",
    "print \"Welcome to Natural Language Processing!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, TensorFlow!\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "sess = tf.Session()\n",
    "print sess.run(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "print sess.run(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Initialized!\n",
      "Step 0 (epoch 0.00), 9.6 ms\n",
      "Minibatch loss: 12.053, learning rate: 0.010000\n",
      "Minibatch error: 90.6%\n",
      "Validation error: 84.6%\n"
     ]
    }
   ],
   "source": [
    "# This is the same as calling python -m (...) on the command line\n",
    "# You should see a bunch of output, and a final test error around 0.8%\n",
    "# It might take a few minutes on a slower machine.\n",
    "%run -m tensorflow.models.image.mnist.convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll interact with TensorFlow as a Python library, but it's really a whole programming system in itself. Continue on to the [**TensorFlow Tutorial Notebook**](TensorFlow%20Tutorial.ipynb) to learn more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "[NLTK](http://www.nltk.org/) is a large compilation of Python NLP packages. It includes implementations of a number of classic NLP models, as well as utilities for working with linguistic data structures, preprocessing text, and managing corpora.\n",
    "\n",
    "NLTK is included with Anaconda, but the corpora need to be downloaded separately. Be warned that this will take up around 3.2 GB of disk space if you download everything! If this is too much, you can download individual corpora as you need them through the same interface.\n",
    "\n",
    "Type the following into a Python shell. It'll open a pop-up UI with the downloader:\n",
    "```\n",
    "import nltk\n",
    "nltk.download()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore the corpora a bit. Let's look at the famous [Brown corpus](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "\n",
      "The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\n",
      "\n",
      "`` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "\n",
      "The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade pip\n",
    "#pip install --upgrade nltk\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Look at the first five sentences\n",
    "for s in brown.sents()[:5]:\n",
    "    print \" \".join(s)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "Fulton\n",
      "County\n",
      "Grand\n",
      "Jury\n",
      "said\n",
      "Friday\n",
      "an\n",
      "investigation\n",
      "of\n",
      "Atlanta's\n",
      "recent\n",
      "primary\n",
      "election\n",
      "produced\n",
      "``\n",
      "no\n",
      "evidence\n",
      "''\n",
      "that\n",
      "any\n",
      "irregularities\n",
      "took\n",
      "place\n",
      ".\n",
      "The\n",
      "jury\n",
      "further\n",
      "said\n",
      "in\n",
      "term-end\n",
      "presentments\n",
      "that\n",
      "the\n",
      "City\n",
      "Executive\n",
      "Committee\n",
      ",\n",
      "which\n",
      "had\n"
     ]
    }
   ],
   "source": [
    "# As words\n",
    "print \"\\n\".join(brown.words()[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also includes a sample of the [Penn treebank](https://www.cis.upenn.edu/~treebank/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> all\n",
      "    Downloading collection u'all'\n",
      "       | \n",
      "       | Downloading package abc to /root/nltk_data...\n",
      "       |   Unzipping corpora/abc.zip.\n",
      "       | Downloading package alpino to /root/nltk_data...\n",
      "       |   Unzipping corpora/alpino.zip."
     ]
    }
   ],
   "source": [
    "# Look at the parse of a sentence.\n",
    "# Don't worry about what this means yet!\n",
    "nltk.download()\n",
    "idx = 45\n",
    "print \" \".join(treebank.sents()[idx])\n",
    "print \"\"\n",
    "print treebank.parsed_sents()[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the [Europarl corpus](http://www.statmt.org/europarl/), which consists of *parallel* text - a sentence and its translations to multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'corpora/europarl_raw/english' not found.  Please use\n  the NLTK Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-923d6a653da4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"ENGLISH: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meuroparl_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menglish\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"FRENCH: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meuroparl_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrench\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/europarl_raw/english' not found.  Please use\n  the NLTK Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "idx = 0\n",
    "\n",
    "print \"ENGLISH: \" + \" \".join(europarl_raw.english.sents()[idx])\n",
    "print \"\"\n",
    "print \"FRENCH: \" + \" \".join(europarl_raw.french.sents()[idx])\n",
    "print \"\"\n",
    "print \"SPANISH: \" + \" \".join(europarl_raw.spanish.sents()[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
